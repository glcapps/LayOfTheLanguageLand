# ü§ñ LLM Impact on Code and Systems

Large Language Models (LLMs) are reshaping how code is written, validated, and even executed. Their influence spans from assistive coding tools to embedded reasoning engines in applications. This article explores three core domains of impact:

---

## üßë‚Äçüíª 1. Code Written *By* LLMs

LLMs like ChatGPT, GitHub Copilot, and others are now common in software development workflows. This raises important questions:

- **What guarantees quality?**
  - LLM output varies in clarity, idiomaticity, and correctness.
  - Linters, formatters, and compilers (especially in statically typed languages like Rust, Go, C#, and TypeScript) are invaluable for surfacing issues.
  - Tests, both unit and property-based, are critical companions to LLM-authored code.

- **What languages handle this well?**
  - **Rust**, **Go**, **Swift**, **Elm** ‚Äî strong compile-time checks and error messages make these languages forgiving of LLM-generated errors.
  - **Python**, **JavaScript**, **Bash** ‚Äî widely used with LLMs but benefit from human review due to looser constraints.

- **Best Practices**
  - Treat LLMs as pair programmers, not single sources of truth.
  - Refactor aggressively. Prompt LLMs to rework code into clearer, modular, or idiomatic forms.
  - Incorporate linting and typing tools into your feedback loop (e.g., `ruff`, `mypy`, `eslint`, `tsc`).

---

## üß† 2. LLMs *Inside* Your Code

More systems now embed LLMs as inference engines. This includes:

- **Chat interfaces** inside apps
- **Agentic workflows** powered by tools like LangChain, Semantic Kernel, or OpenAI Assistants
- **Natural language querying** over databases or file systems

Tools like **Semantic Kernel (C# and Python)**, **LangChain (Python and JavaScript)**, and **Haystack (Python)** offer framework-level support for embedding reasoning and tool-using agents. These tools abstract prompt orchestration, memory integration, tool invocation, and multi-turn reasoning flows across multiple runtime environments.

This shifts architecture toward:
- Dynamic prompt construction
- Context management and summarization
- Output validation, parsing, and safety checking

It also creates new "types of bugs" ‚Äî hallucinations, prompt drift, and tool misuse ‚Äî that developers must reason about.

---

## üîß 3. Consuming LLM Toolchains and Protocols

Developers increasingly interact with LLMs via:

- **Tool Use Interfaces** (e.g. OpenAI tool calling, Function calling, ReAct format)
- **MCP (Model-Context Protocol)** ‚Äî a formalized request-response contract for agent orchestration
- **Function Schemas** ‚Äî declarative ways of describing actions, inputs, and outputs for LLM interaction

These protocols require:

- Thoughtful schema design (naming, typing, required fields)
- Declarative intent encoding
- Semantic fallbacks (e.g. if a tool fails, replan the conversation)

---

## üß¨ 4. Model Creation and Fine-Tuning

While most developers consume LLMs via APIs or SDKs, some teams engage in model creation, fine-tuning, or embedding domain-specific knowledge into models. This space remains heavily influenced by Python due to its dominant ecosystem of ML libraries, but the patterns are noteworthy for all languages.

- **Python leads here** with toolkits like HuggingFace Transformers, PyTorch, TensorFlow, and OpenLLM.
- **Fine-tuning workflows** involve prompt formatting, dataset curation, tokenizer management, and adapter layers (e.g. LoRA).
- **Deployment concerns** include quantization, GPU memory targeting, and inference latency.

While not many generalist developers write training loops, more are engaging in:
- **Embedding tuning** for vector search and hybrid RAG systems
- **System prompts and adapters** that constrain model behavior in production
- **Evaluation metrics and bias testing** that influence model adoption

Other languages like Rust (for inference speed), Go (infrastructure glue), and C++ (for runtime customization) occasionally appear ‚Äî but Python remains the operational lingua franca.

---

## üß≠ Summary: The New Dev Stack

| Area                            | LLM-Driven Shift                              |
|---------------------------------|-----------------------------------------------|
| Coding                         | Auto-generation, refactoring, tests-as-prompts |
| System Design                  | LLMs as reasoning components, not just inputs |
| Language Choice                | Favoring strongly-typed or tool-rich languages |
| Tool Protocols                 | Interfaces like OpenAI tools, MCP, JSON schemas |
| Debugging                      | New mental models for tracing inference failures |

LLMs don‚Äôt replace coders ‚Äî they reroute attention. They make syntax easy, but **architecture, correctness, and intent** matter more than ever.

---

## üè∑Ô∏è Languages Most Impacted in Each Area

| Area               | Noteworthy Languages                                                                 |
|--------------------|--------------------------------------------------------------------------------------|
| Code Gen           | Python, JavaScript, TypeScript, Bash, Rust, Go, C#                                   |
| System Embedding   | Python (LangChain, Haystack), JavaScript/TypeScript (Next.js, LangChain.js), C# (Semantic Kernel) |
| Toolchain Schemas  | JSON Schema, TypeScript, C# with decorators/attributes                               |
| Compile+Lint Guard | Rust, Go, Elm, TypeScript, C#, Swift                                                  |
| Prompt-as-Logic    | YAML (flows), Markdown (prompt structure), JSON (structured prompts)                 |

This adds language associations per impact domain, highlighting which languages benefit from or shape these LLM-driven transformations.